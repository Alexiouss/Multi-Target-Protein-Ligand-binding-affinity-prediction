# Multi-Target Proteinâ€“Ligand Binding Affinity Prediction

This repository contains **classical and hybrid quantumâ€“classical models** for **multi-target proteinâ€“ligand binding affinity prediction**, with a focus on **attention mechanisms** and their feasibility in the **NISQ era**.

The work is developed as part of a **diploma thesis** and is intended as a **research prototype**, not a production system.

---

## ðŸ”¬ Research Questions

This project investigates the following questions:

- **Can quantum attention mechanisms be meaningfully applied to multi-target binding affinity prediction under NISQ constraints?**
- **How much does attention matter** in a ligand-centric, multi-target setting?
- How do **quantum attention variants** compare to **classical attention** when all other components are kept identical?

The emphasis is on **architectural comparison, feasibility, and interpretability**, rather than leaderboard performance.

---

## ðŸ§  High-Level Architecture

### Ligand Representation
- SMILES strings are converted into molecular graphs.
- Graph encoders:
  - GCN
  - GINE
- Output: fixed-size molecular embeddings shared across all targets.

### Protein Representation
- Proteins are **not explicitly encoded** in the current version.
- Instead, **learned target embeddings** act as placeholders.
- **Explicit protein encoding** (e.g. residue-level representations) is planned as future work.

### Attention Mechanisms

Two attention variants are implemented:

#### Classical Attention
- Standard QKV attention
- Implemented using PyTorchâ€™s attention primitives
- Serves as the baseline

#### Quantum Attention
- Parameterized Quantum Circuits (PQCs)
- Angle encoding and amplitude encoding
- Optional data re-uploading
- Per-target quantum circuits
- Fully differentiable via PennyLane

Both variants share:
- The same graph encoder
- The same prediction heads
- The same training and evaluation pipeline

---

## ðŸ§© Project Structure

```text
src/
 â”œâ”€ data_loader.py
 â”œâ”€ data_loader_chembl.py
 â”œâ”€ graph_encoder.py
 â”œâ”€ graph_encoder_gin.py
 â”œâ”€ quantum_attention_refactored.py
 â”œâ”€ multi_target_model_gcn_refactored.py
 â”œâ”€ multi_target_model_multiple_models_gine.py
 â”œâ”€ preprocess_dataset.py
 â”œâ”€ fit_pca_unified.py
 â”œâ”€ draw_circuits.py
 â”œâ”€ visualize_attention_space.py

scripts/
 â”œâ”€ train.py
 â”œâ”€ test.py
 â”œâ”€ application.py
 â”œâ”€ app_ui.py
 â””â”€ analysis/
    â”œâ”€ quantum_training_diagnostics.py
    â”œâ”€ plot_quantum_correlations.py
    â”œâ”€ plot_bloch_vectors.py
    â”œâ”€ plot_bitstrings_probs.py
    â”œâ”€ interpret_target_ablation.py
    â”œâ”€ interpret_attention_memory.py
    â”œâ”€ analyze_pca_embeddings.py
    â”œâ”€ analyze_tsne_embeddings.py
    â””â”€ run_all_analysis.py

data/
 â””â”€ chembl/
    â””â”€ chembl_affinity_dataset.csv

results/
