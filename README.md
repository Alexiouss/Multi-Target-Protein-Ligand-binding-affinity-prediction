# Multi-Target Proteinâ€“Ligand Binding Affinity Prediction

This repository contains **classical and hybrid quantumâ€“classical models** for **multi-target proteinâ€“ligand binding affinity prediction**, with a focus on **attention mechanisms** and their feasibility in the **NISQ era**.

The work is developed as part of a **diploma thesis** and is intended as a **research prototype**, not a production system.

---

## ğŸ”¬ Research Questions

This project investigates the following questions:

- **Can quantum attention mechanisms be meaningfully applied to multi-target binding affinity prediction under NISQ constraints?**
- **How much does attention matter** in a ligand-centric, multi-target setting?
- How do **quantum attention variants** compare to **classical attention** when all other components are kept identical?

The emphasis is on **architectural comparison, feasibility, and interpretability**, rather than leaderboard performance.

---

## ğŸ§  High-Level Architecture

### Ligand Representation
- SMILES strings are converted into molecular graphs.
- Graph encoders:
  - GCN
  - GINE
- Output: fixed-size molecular embeddings shared across all targets.

### Protein Representation
- Proteins are **not explicitly encoded** in the current version.
- Instead, **learned target embeddings** act as placeholders.
- **Explicit protein encoding** (e.g. residue-level representations) is planned as future work.

### Attention Mechanisms

Two attention variants are implemented:

#### Classical Attention
- Standard QKV attention
- Implemented using PyTorchâ€™s attention primitives
- Serves as the baseline

#### Quantum Attention
- Parameterized Quantum Circuits (PQCs)
- Angle encoding and amplitude encoding
- Optional data re-uploading
- Per-target quantum circuits
- Fully differentiable via PennyLane

Both variants share:
- The same graph encoder
- The same prediction heads
- The same training and evaluation pipeline

---

## ğŸ§© Project Structure

```text
src/
 â”œâ”€ data_loader.py
 â”œâ”€ data_loader_chembl.py
 â”œâ”€ graph_encoder.py
 â”œâ”€ graph_encoder_gin.py
 â”œâ”€ quantum_attention_refactored.py
 â”œâ”€ multi_target_model_gcn_refactored.py
 â”œâ”€ multi_target_model_multiple_models_gine.py
 â”œâ”€ preprocess_dataset.py
 â”œâ”€ fit_pca_unified.py
 â”œâ”€ draw_circuits.py
 â”œâ”€ visualize_attention_space.py

scripts/
 â”œâ”€ train.py
 â”œâ”€ test.py
 â”œâ”€ application.py
 â”œâ”€ app_ui.py
 â””â”€ analysis/
    â”œâ”€ quantum_training_diagnostics.py
    â”œâ”€ plot_quantum_correlations.py
    â”œâ”€ plot_bloch_vectors.py
    â”œâ”€ plot_bitstrings_probs.py
    â”œâ”€ interpret_target_ablation.py
    â”œâ”€ interpret_attention_memory.py
    â”œâ”€ analyze_pca_embeddings.py
    â”œâ”€ analyze_tsne_embeddings.py
    â””â”€ run_all_analysis.py

data/
 â””â”€ chembl/
    â””â”€ chembl_affinity_dataset.csv

results/
```
## ğŸ“Š Datasets
### Synthetic Dataset
-Generated on-the-fly
-Fully configurable via config.json
-Safe to regenerate for every run
### ChEMBL-based Dataset
-Preprocessed CSV format
-Multi-target binding affinity data
-Targets are automatically derived from the dataset
## âš™ï¸ Configuration
Two configuration files are used:
```text
-config.json
```
Full experiment configuration (targets, data, model, training).
```text
config_ui_run.json
```
Automatically generated when running experiments via the UI.
## â–¶ï¸ Running the Project

### Install dependencies
First, install the required Python dependencies:

```bash
pip install -r requirements.txt
```
### CUDA support (optional but recommended)
This project supports **GPU acceleration via CUDA**, but CUDA must be installed independently by the user, according to their system, GPU, and NVIDIA driver version.
To enable CUDA support:
1. Ensure you have a compatible NVIDIA GPU.
2. Install the appropriate NVIDIA drivers for your system.
3. Install a CUDA-enabled version of PyTorch that matches your CUDA runtime.
For example, for CUDA 11.8:
```bash
pip install --index-url https://download.pytorch.org/whl/cu118 torch torchvision torchaudio
```
### Launch the application
The entire pipeline is orchestrated from:
```text
streamlit run scripts/app_ui.py
```
## ğŸ§ª Training & Testing
-Training and testing are explicitly separated
-No pretrained models are provided
-All results must be reproduced by the user
## âš ï¸ Limitations
-Research prototype, not production code
-Quantum execution is simulated
-Protein structure and sequence are not yet encoded
-Computational cost grows rapidly with qubits
## ğŸ”® Future Work
-Explicit protein encoding
-Moleculeâ€“residue attention
-Hardware-aware quantum circuits
-Execution on real quantum hardware
## ğŸ“– Citation
If you use this code in academic work, please cite the diploma thesis:
```text
@mastersthesis{AlexandrosZacharakis2026QuantumDrugDiscovery,
  title  = {Quantum machine learning algorithms and their applications},
  author = {Alexandros Zacharakis},
  year   = {2026},
  school = {University of Patras}
}
```
## ğŸ“œ License
This project is released under the MIT License, allowing academic and research use with attribution.
